# Polkadot AI Chatbot

An AI-powered chatbot system that provides intelligent answers about the Polkadot ecosystem using RAG (Retrieval-Augmented Generation) with OpenAI embeddings and ChromaDB.

## Features

- üìö **Knowledge Base**: Built from Polkadot Wiki and Forum data
- üîç **Semantic Search**: Uses OpenAI embeddings for accurate document retrieval
- ü§ñ **AI-Powered Answers**: Generates contextual responses using GPT models
- üöÄ **Fast API**: RESTful API with automatic documentation
- üíæ **Persistent Storage**: ChromaDB for efficient vector storage
- üîß **Configurable**: Environment-based configuration
- üìä **Analytics**: Built-in statistics and monitoring

## Project Structure

```
polkassembly-ai/
‚îú‚îÄ‚îÄ README.md                  # Project documentation
‚îú‚îÄ‚îÄ env.example                # Environment variables template
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ create_embeddings.py       # Entry point to generate embeddings (run once)
‚îú‚îÄ‚îÄ run_server.py              # Entry point for API server
‚îú‚îÄ‚îÄ run_tests.py               # Entry point for testing
‚îú‚îÄ‚îÄ data/                      # Data sources
‚îÇ   ‚îî‚îÄ‚îÄ data_sources/
‚îÇ       ‚îú‚îÄ‚îÄ polkadot_network/  # Forum data (JSON/TXT files)
‚îÇ       ‚îî‚îÄ‚îÄ polkadot_wiki/     # Wiki data (TXT files)
‚îî‚îÄ‚îÄ src/                       # Source code directory
    ‚îú‚îÄ‚îÄ rag/                   # RAG application modules
    ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
    ‚îÇ   ‚îú‚îÄ‚îÄ create_embeddings.py # Embedding generation logic
    ‚îÇ   ‚îî‚îÄ‚îÄ api_server.py      # FastAPI server
    ‚îú‚îÄ‚îÄ utils/                 # Utility modules
    ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py     # Document loading and processing
    ‚îÇ   ‚îú‚îÄ‚îÄ text_chunker.py    # Text chunking for embeddings
    ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py      # OpenAI embeddings and ChromaDB
    ‚îÇ   ‚îî‚îÄ‚îÄ qa_generator.py    # Q&A generation with OpenAI
    ‚îî‚îÄ‚îÄ test/                  # Testing modules
        ‚îî‚îÄ‚îÄ test_api.py        # API testing script
```

## Setup Instructions

### 1. Clone and Install

```bash
# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Configuration

```bash
# Copy the example environment file
cp env.example .env

# Edit .env with your OpenAI API key
# OPENAI_API_KEY=your_actual_openai_api_key_here
```

### 3. Create Embeddings (One-time Setup)

This step processes all documents and creates embeddings:

```bash
# Generate embeddings and store in ChromaDB
python create_embeddings.py

# Optional: Clear existing embeddings and recreate
python create_embeddings.py --clear

# Optional: Customize batch size and token limits
python create_embeddings.py --batch-size 25 --min-tokens 100 --max-tokens 800
```

### 4. Start the API Server

```bash
# Start the FastAPI server
python run_server.py

# Or using uvicorn directly
uvicorn src.rag.api_server:app --host 0.0.0.0 --port 8000
```

### 5. (Optional) Configure Memory

To enable conversation memory with Mem0:

```bash
# Install mem0 package
pip install mem0ai

# Add your Mem0 API key to .env file
echo "MEM0_API_KEY=your_mem0_api_key_here" >> .env
```

### 6. Test the System

```bash
# Run all tests
python run_tests.py

# Run specific tests
python run_tests.py --api        # API functionality only
python run_tests.py --memory     # Memory integration only
python run_tests.py --formatting # Clean formatting validation
```

## API Endpoints

### üè• Health Check
```http
GET /health
```

### ü§ñ Ask Questions
```http
POST /query
Content-Type: application/json

{
  "question": "What is Polkadot?",
  "max_chunks": 5,
  "include_sources": true,
  "custom_prompt": "optional custom system prompt"
}
```

### üîç Search Documents
```http
POST /search
Content-Type: application/json

{
  "query": "staking rewards",
  "n_results": 10,
  "source_filter": "polkadot_wiki"
}
```

### üìä Get Statistics
```http
GET /stats
```

## üß† Memory Integration

The system includes **conversation memory** powered by Mem0, enabling context-aware conversations:

### Features
- **Context Retention**: Remembers previous questions and answers
- **Smart Follow-ups**: Uses conversation history for better responses
- **Automatic Memory**: No manual memory management required
- **Privacy**: Isolated memory per user session

### Memory Flow
1. **User Query**: System searches memory for relevant context
2. **Context Injection**: Memory context is added to prompt
3. **Response Generation**: AI considers both documents and memory
4. **Memory Storage**: Query and response are automatically stored

### Example Conversation
```
User: "What is staking in Polkadot?"
Bot: "Staking in Polkadot allows DOT holders to..."

User: "What are the rewards for that?"
Bot: "Staking rewards in Polkadot include..." # Uses memory context
```

## Usage Examples

### Python Client Example

```python
import requests

# Ask a question
response = requests.post("http://localhost:8000/query", json={
    "question": "How do I stake DOT tokens?",
    "max_chunks": 3,
    "include_sources": True
})

data = response.json()
print(f"Answer: {data['answer']}")
print(f"Confidence: {data['confidence']}")
print(f"Sources: {len(data['sources'])}")
```

### cURL Examples

```bash
# Health check
curl http://localhost:8000/health

# Ask a question
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are parachains?",
    "max_chunks": 3,
    "include_sources": true
  }'

# Search documents
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "governance voting",
    "n_results": 5
  }'
```

## Configuration Options

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | - | Your OpenAI API key (required) |
| `OPENAI_MODEL` | gpt-3.5-turbo | OpenAI model for answers |
| `OPENAI_EMBEDDING_MODEL` | text-embedding-ada-002 | Embedding model |
| `MEM0_API_KEY` | - | Mem0 API key for conversation memory (optional) |
| `WEB_SEARCH` | true | Enable web search fallback |
| `WEB_SEARCH_CONTEXT_SIZE` | high | Web search context size (low/medium/high) |
| `CHROMA_PERSIST_DIRECTORY` | ./chroma_db | ChromaDB storage path |
| `CHROMA_COLLECTION_NAME` | polkadot_embeddings | Collection name |
| `API_HOST` | 0.0.0.0 | API server host |
| `API_PORT` | 8000 | API server port |
| `CHUNK_SIZE` | 1000 | Text chunk size for embeddings |
| `CHUNK_OVERLAP` | 200 | Overlap between chunks |

## Data Sources

The system processes two types of data:

1. **Polkadot Wiki** (`data/data_sources/polkadot_wiki/`)
   - Technical documentation
   - Guides and tutorials
   - Architecture information

2. **Polkadot Forum** (`data/data_sources/polkadot_network/`)
   - Community discussions
   - Governance proposals
   - Ambassador program information

## API Documentation

Once the server is running, visit:
- **Interactive API Docs**: http://localhost:8000/docs
- **ReDoc Documentation**: http://localhost:8000/redoc

## Monitoring and Logs

- Embedding creation logs: `embedding_creation.log`
- API server logs: Console output
- ChromaDB storage: `./chroma_db/` directory

## Troubleshooting

### Common Issues

1. **"OPENAI_API_KEY must be set"**
   - Ensure your `.env` file contains a valid OpenAI API key

2. **"No data available. Please create embeddings first."**
   - Run `python create_embeddings.py` to initialize the database

3. **"Collection already exists with data"**
   - Use `--clear` flag to recreate embeddings: `python create_embeddings.py --clear`

4. **Rate limiting from OpenAI**
   - The system includes automatic retry logic
   - Consider reducing batch size: `--batch-size 10`

### Performance Tips

- **Faster embedding creation**: Increase `--batch-size` (but watch rate limits)
- **Memory optimization**: Reduce `CHUNK_SIZE` in configuration
- **Better accuracy**: Increase `max_chunks` in queries (but slower)

## Development

### Project Structure

The project uses a clean, modular structure:
- **Root level**: Contains entry point scripts and documentation
- **`src/rag/`**: Contains RAG application logic (config, embeddings, API)
- **`src/utils/`**: Contains utility modules for data processing
- **`src/test/`**: Contains testing modules

### Adding New Data Sources

1. Place new documents in `data/data_sources/polkadot_network/` or `data/data_sources/polkadot_wiki/`
2. Run `python create_embeddings.py` to process new documents
3. The system will automatically detect and process new files

### Customizing the AI

- Modify prompts in `src/utils/qa_generator.py`
- Adjust embedding parameters in `src/utils/embeddings.py`
- Change chunking strategy in `src/utils/text_chunker.py`
- Update configuration in `src/rag/config.py`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with `python run_tests.py`
5. Submit a pull request

## License

This project is licensed under the MIT License.

## Support

For issues and questions:
1. Check the troubleshooting section
2. Review the API documentation at `/docs`
3. Check logs for error details